{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7fd0b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "src_path = os.path.join(current_dir, '..', 'src')\n",
    "if os.path.exists(src_path):\n",
    "    sys.path.append(src_path)\n",
    "else:\n",
    "    # Try alternative path structure\n",
    "    alt_src_path = os.path.join(current_dir, 'src')\n",
    "    if os.path.exists(alt_src_path):\n",
    "        sys.path.append(alt_src_path)\n",
    "    else:\n",
    "        print(\"Warning: Could not find src directory. Please ensure the path is correct.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e1557c",
   "metadata": {},
   "source": [
    "# RAG Pipeline Demo\n",
    "\n",
    "This notebook demonstrates the Retrieval-Augmented Generation (RAG) pipeline for analyzing financial complaints. The pipeline combines:\n",
    "\n",
    "1. **Retrieval**: Uses FAISS vector search to find relevant complaint chunks\n",
    "2. **Generation**: Leverages a language model (flan-t5-base) to generate answers\n",
    "3. **Context Integration**: Combines retrieved context with user questions\n",
    "\n",
    "## What's Next\n",
    "\n",
    "The following cell will:\n",
    "- Load the pre-built embedding indexer with FAISS vectors\n",
    "- Initialize the RAG pipeline with the language model\n",
    "- Test the complete pipeline with a sample question\n",
    "- Display the generated answer and retrieved source chunks\n",
    "\n",
    "This demonstrates the end-to-end functionality of our complaint analysis system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef208073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b303a384dbbc46dbb315909fe7700b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f918b613e640c18a2db90c1e00d0a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c1f70b9c7b94ae4839311f266c748b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbeb887c7b5a4edd9ac01fa4cb260f1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d18000d106294c8dbffc9201f6a5899e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f2217dfe08241c99ba46a8b8d0291eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are common issues with credit card complaints?\n",
      "Generated Answer: a deceptive and inconsistent interest charge on one of my Citi credit cards, despite paying the full statement balance during the billing cycle and having a ---\n",
      "Top Retrieved Sources:\n",
      "- is the credit card department, not the trouble dep ...\n",
      "- I am submitting this complaint regarding a decepti ...\n"
     ]
    }
   ],
   "source": [
    "from embedding_indexing import create_embedding_indexer\n",
    "from rag_pipeline import RAGPipeline\n",
    "# Load the indexer\n",
    "indexer = create_embedding_indexer()\n",
    "indexer.load_faiss_index(os.path.join(current_dir, \"..\", \"vector_store\"))\n",
    "\n",
    "# Initialize the RAG pipeline\n",
    "rag = RAGPipeline(indexer=indexer, llm_model=\"google/flan-t5-base\", device=-1, top_k=5)\n",
    "\n",
    "# Example question\n",
    "question = \"What are common issues with credit card complaints?\"\n",
    "result = rag.answer_question(question)\n",
    "\n",
    "print(\"Question:\", result['question'])\n",
    "print(\"Generated Answer:\", result['answer'])\n",
    "print(\"Top Retrieved Sources:\")\n",
    "for src in result['retrieved_sources']:\n",
    "    print(\"-\", src['text'][:50], \"...\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c205fea1",
   "metadata": {},
   "source": [
    "## Testing the RAG Pipeline \n",
    "**The next cell will thoroughly test our RAG pipeline with multiple scenarios:**\n",
    "# \n",
    "1. **Comprehensive Question Testing**\n",
    "**We'll test the pipeline with 5 different types of questions covering various aspects of financial complaints:**\n",
    " - General complaint patterns\n",
    " - Resolution processes  \n",
    " - Technology-related issues\n",
    " - Performance metrics\n",
    " - Security concerns\n",
    "# \n",
    "2. **Component-Level Testing**\n",
    "**We'll break down the pipeline to test each component individually:**\n",
    " - **Retrieval**: Test the FAISS vector search functionality\n",
    " - **Prompt Engineering**: Verify the context integration\n",
    " - **Generation**: Test the LLM response generation\n",
    "# \n",
    "3. **Performance Evaluation**\n",
    "**We'll measure the end-to-end processing time to assess system performance.**\n",
    "# \n",
    "- This testing approach helps us understand both the overall pipeline performance and identify any bottlenecks in individual components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52488d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RAG Pipeline Evaluation ===\n",
      "\n",
      "Question 1: What are the most common customer complaints?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: lack of growth, lack of transparency, and lack of basic customer appreciation\n",
      "Retrieved Sources: 2 chunks\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question 2: How do customers typically resolve billing disputes?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: They file a dispute on XX/XX/XXXX, after learning the attorney was disbarred, never provided any billing records, and failed to deliver the services promised ---\n",
      "Retrieved Sources: 2 chunks\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question 3: What issues do customers face with mobile banking?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: lack of transparency, accountability, and customer service\n",
      "Retrieved Sources: 2 chunks\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question 4: What are the typical resolution times for complaints?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 7-10 business days, then I call again and admit was not so nice, they told me another 10 days, I call today and again another 10 days\n",
      "Retrieved Sources: 2 chunks\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question 5: How do customers report fraud or suspicious activity?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: a fraud alert on it without first resolving the fraud alert\n",
      "Retrieved Sources: 2 chunks\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Testing Individual Components ===\n",
      "\n",
      "Retrieved 5 chunks for: 'What are common credit card issues?'\n",
      "Chunk 1: is the credit card department, not the trouble department ''...\n",
      "Chunk 2: . Over the next 4 years and 6 months, I maintained a very good to excellent credit score, never missed a payment, and consistently used both cards res...\n",
      "\n",
      "Generated prompt length: 1114 characters\n",
      "Prompt preview: You are a financial analyst assistant for CrediTrust. Your task is to answer questions about customer complaints. Use the following retrieved complaint excerpts to formulate your answer. If the contex...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated answer: Credit card companies and banks have enormous power and consumers are left with situations like this. They should be REQUIRED to let you know at the Point of Sale what the problem is.\n",
      "\n",
      "=== Performance Metrics ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total processing time: 4.75 seconds\n",
      "Retrieved chunks: 2\n",
      "Answer length: 77 characters\n"
     ]
    }
   ],
   "source": [
    "# Test the RAG pipeline with different types of questions\n",
    "test_questions = [\n",
    "    \"What are the most common customer complaints?\",\n",
    "    \"How do customers typically resolve billing disputes?\",\n",
    "    \"What issues do customers face with mobile banking?\",\n",
    "    \"What are the typical resolution times for complaints?\",\n",
    "    \"How do customers report fraud or suspicious activity?\"\n",
    "]\n",
    "\n",
    "print(\"=== RAG Pipeline Evaluation ===\\n\")\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"Question {i}: {question}\")\n",
    "    result = rag.answer_question(question)\n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "    print(f\"Retrieved Sources: {len(result['retrieved_sources'])} chunks\")\n",
    "    print(\"-\" * 80 + \"\\n\")\n",
    "\n",
    "# Test individual pipeline components\n",
    "print(\"=== Testing Individual Components ===\\n\")\n",
    "\n",
    "# Test retrieval only\n",
    "test_question = \"What are common credit card issues?\"\n",
    "retrieved_chunks = rag.retrieve(test_question)\n",
    "print(f\"Retrieved {len(retrieved_chunks)} chunks for: '{test_question}'\")\n",
    "for i, chunk in enumerate(retrieved_chunks[:2], 1):\n",
    "    print(f\"Chunk {i}: {chunk['text'][:150]}...\")\n",
    "\n",
    "# Test prompt building\n",
    "prompt = rag.build_prompt(test_question, retrieved_chunks)\n",
    "print(f\"\\nGenerated prompt length: {len(prompt)} characters\")\n",
    "print(f\"Prompt preview: {prompt[:200]}...\")\n",
    "\n",
    "# Test generation only\n",
    "generated_answer = rag.generate(prompt)\n",
    "print(f\"\\nGenerated answer: {generated_answer}\")\n",
    "\n",
    "# Performance evaluation\n",
    "print(\"\\n=== Performance Metrics ===\")\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "result = rag.answer_question(\"What are common customer complaints?\")\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Total processing time: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Retrieved chunks: {len(result['retrieved_sources'])}\")\n",
    "print(f\"Answer length: {len(result['answer'])} characters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7acc9b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| Question | Generated Answer | Retrieved Sources | Quality Score (1-5) | Comments/Analysis |\n",
       "|---|---|---|---|---|\n",
       "| What are the common issues customers face? | They are not satisfied with the service they receive. |  |  |  |\n",
       "| How can I resolve a complaint about delayed service? | Ask for a refund. |  |  |  |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run qualitative evaluation\n",
    "results = RAGPipeline.qualitative_evaluation(rag_pipeline, questions)\n",
    "\n",
    "# Generate evaluation table\n",
    "evaluation_table = RAGPipeline.evaluation_table(results)\n",
    "\n",
    "# Display the evaluation table\n",
    "display(Markdown(evaluation_table))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
